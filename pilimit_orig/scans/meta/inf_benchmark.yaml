description: sweep InfNet MAML

target:
  service: amlk8s
  name: ms-shared

environment:
  image: elutece/philly_fairseq_cuda10:latest
  setup:
    # NOTE: the torch install will cause some MKL_THREADING_LAYER error that arises whenever python is called, so we must install everything all at once and the next call must start with `export MKL_THREADING_LAYER=GNU;`
     - pip install torch==1.5.1 torchvision==0.6.1 cox dill tables h5py torchmeta==1.5.1 --user
code:
  # local directory of the code. this will be uploaded to the server.
  # $CONFIG_DIR is expanded to the directory of this config file
  local_dir: $CONFIG_DIR/../../
storage:
  output:
    # storage_account_name: gregyangstorage
    storage_account_name: inflimitmsr
    container_name: teamdrive
    is_output: True
# list of jobs to run, we run 2 jobs in this example
search:
  # name must be unique across the jobs
  job_template:
    name: pinet-sweep-maml
    sku: G1-V100
    command:
    - export MKL_THREADING_LAYER=GNU; bash scans/meta/inf_benchmark_driver.sh
      --step_size {step_size}
      --meta_lr {meta_lr}
      --grad_clip {grad_clip}
      --bias_alpha {bias_alpha}
      --first_layer_alpha {first_layer_alpha}
      --first_layer_lr_mult {first_layer_lr_mult}
      --savedir $$PT_OUTPUT_DIR
    # python -m meta.train dataset --dataset omniglot --num-ways 5 --num-shots 1 --use-cuda --step-size 0.5 --batch-size 8 --num-workers 2 --num-epochs 2 --output-folder results --meta-lr 16  --grad-clip 0.1 --meta-momentum 0 --num-shots-test 1 --normalize None --hidden-size -1 --bias-alpha 4 --infnet_r 200 --first-layer-alpha 2 --depth 2 --verbose --first-layer-lr-mult 0.2 --dtype float16 --num-batches 1000 --num-test-batches 500 --adapt-readout-only --Gproj-inner --Gproj-outer --last-layer-lr-mult 0 --scheduler cosine --readout-zero-init
    # python -c 'import torch; print(torch.__version__)'
  type: random
  max_trials: 1
  params:
    # fine
    - name: step_size
      spec: discrete
      values: "0.5 * 2.**np.arange(-2.5, .75, 0.25)"
    - name: meta_lr
      spec: discrete
      values: "8 * 2.**np.arange(0, 2, 0.25)"
    - name: grad_clip
      spec: discrete
      values: "0.1 * 2.**np.arange(.5, 2.5, 0.25)"
    - name: bias_alpha
      spec: discrete
      values: "4 * 2.**np.arange(-2.5, 1, 0.5)"
    - name: first_layer_alpha
      spec: discrete
      values: "2 * 2.**np.arange(-4, .5, 0.5)"
    - name: first_layer_lr_mult
      spec: discrete
      values: "0.2 * 2.**np.arange(-.5, 2, 0.25)"

    # coarse
    # - name: step_size
    #   spec: discrete
    #   values: "0.5 * 2.**np.arange(-2, 2, 0.5)"
    # - name: meta_lr
    #   spec: discrete
    #   values: "16 * 2.**np.arange(-2, 2, 0.5)"
    # - name: grad_clip
    #   spec: discrete
    #   values: "0.1 * 2.**np.arange(-2, 2, 0.5)"
    # - name: bias_alpha
    #   spec: discrete
    #   values: "4 * 2.**np.arange(-2, 2, 0.5)"
    # - name: first_layer_alpha
    #   spec: discrete
    #   values: "2 * 2.**np.arange(-2, 2, 0.5)"
    # - name: first_layer_lr_mult
    #   spec: discrete
    #   values: "0.2 * 2.**np.arange(-2, 2, 0.5)"
